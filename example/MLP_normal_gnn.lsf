#!/bin/bash
#BSUB -J mlp ##job name
#BSUB -q gpu-phy-zhangwq ##queue name
#BSUB -n 8 ##number of total cpu cores
#BSUB -e %J.err
#BSUB -o %J.out
#BSUB -gpu "num=1/host"
#BSUB -R "span[ptile=128]" ## cpu core num on each host

conda activate FAMS-DNN
unzip -q -o dataset.zip
python my_run_train.py \
    --gate=None \
    --device="cuda" \
    --train_file=dataset/train_data0.xyz \
    --test_file=dataset/test_data0.xyz \
    --valid_file=dataset/valid_data0.xyz \
    --name=gnn \
    --E0s="average" \
    --loss='universal' \
    --energy_weight=1 \
    --forces_weight=10 \
    --compute_stress=True \
    --energy_key="energy" \
    --forces_key="forces" \
    --eval_interval=1 \
    --virials_weight=100 \
    --virials_key='virial' \
    --eval_interval=1 \
    --error_table='PerAtomMAE' \
    --model="FAMS_DNN" \
    --interaction_first="Residual_InteractionBlock" \
    --interaction="Residual_InteractionBlock" \
    --num_recursions=0 \
    --product=3 \
    --max_ell=3 \
    --r_max=6.0 \
    --max_L=0 \
    --num_channels=128 \
    --num_radial_basis=10 \
    --MLP_irreps="16x0e" \
    --scaling='rms_forces_scaling' \
    --num_workers=16 \
    --lr=0.005 \
    --weight_decay=1e-8 \
    --ema \
    --ema_decay=0.995 \
    --scheduler_patience=5 \
    --batch_size=16 \
    --valid_batch_size=16 \
    --max_num_epochs=10 \

